---
title: "Boosted Tree"
author: "Chris Ives"
date: "11/2/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
needs(rio, fs)
git <- "~/Documents/GitHub/EDLD-654-Final"

```

## Read in the `train.csv` data. Please feel free to use `sample_frac()` if you find that the data file is too large for your machine.

```{r, data}
data <- import(path(git, "data/train.csv")) %>% 
  select(-classification) %>%
  mutate_if(is.character, factor) %>% 
  sample_frac(.05)

bonus <- import(path(git, "data/bonus_data.csv"))

data <- data %>% 
  left_join(bonus)
```


## 1. Initial Split

Set a seed and split the data into a training set and a testing set as two named objects. 

```{r, initial_split}
set.seed(3000)

data_split <- initial_split(data)

train <- training(data_split)

test <- testing(data_split)

rm(data)

```


``` {r}
rec <- recipe(score ~ ., train) %>%
step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt)),
            lang_cd = case_when(lang_cd == "S" ~ "S", TRUE ~ "E"),
            pupil_tch_ratio = as.numeric(pupil_tch_ratio)) %>% 
step_rm(contains("id"), ncessch, ncesag, lea_name, sch_name) %>%
step_zv(all_predictors()) %>%
step_unknown(all_nominal()) %>% 
step_medianimpute(all_numeric()) %>% 
step_dummy(all_nominal())

baked_train <- prep(rec) %>% 
  bake(train)

baked_test <- prep(rec) %>% 
  bake(test)


library(xgboost)
library(ggplot2)
library(reshape2)
library(data.table)
needs(caret)

needs(Ecdat)
 
set.seed(1)
N = 1000
k = 10
x = matrix(rnorm(N*k),N,k)
b = (-1)^(1:k)
yaux=(x%*%b)^2
e = rnorm(N)
y=yaux+e

train_x = data.matrix(baked_train[, -46])
train_y = data.matrix(baked_train[, 46])
test_x = data.matrix(baked_test[, -46])
test_y = data.matrix(baked_test[, 46])

xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)

xgbc = xgboost(data = xgb_train, max.depth = 2, nrounds = 50)


pred_y = predict(xgbc, xgb_test)
rmse = caret::RMSE(test_y, pred_y)

mat <- xgb.importance (feature_names = colnames(train_x),model = xgbc)
xgb.plot.importance (importance_matrix = mat[1:20]) 

def_mod = xgb.cv(data = train_x,
                nfold = 3,
                label = train_y,
                nrounds = 100,
                verbose = FALSE,
                objective = "reg:squarederror",
                eval_metric = "rmse")

```

``` {r}
def_mod$evaluation_log[def_mod$best_iteration, ]

pull_eval <- function(m) {
  m[["evaluation_log"]] %>% 
    pivot_longer(-iter,
                 names_to = c("set", NA, "stat"),
                 names_sep = "_",
                 values_to = "val") %>% 
    pivot_wider(names_from = "stat", 
                values_from = "val") 
}

def_mod %>% 
  pull_eval() %>% 
  filter(iter > 7) %>% 
  ggplot(aes(iter, mean, color = set)) +
  geom_line() +
  geom_point()

######### Train learning rate 
lr <- seq(0.0001, 0.3, length.out = 30)

lr_mods <- map(lr, function(learn_rate) {
  xgb.cv(
    data = train_x,
    label = train_y,
    nrounds = 5000,
    objective = "reg:linear",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 0,
    params = list( 
      eta = learn_rate,
      nthread = 4
    ) 
  )  
}) 

names(lr_mods) <- lr
evals_lr <- map_df(lr_mods, pull_eval, .id = "learning_rate")

rmse_lr <- evals_lr %>% 
  group_by(learning_rate, set) %>% 
  summarize(min = min(mean)) %>% 
  pivot_wider(names_from = set, values_from = min) %>% 
  arrange(test)

rmse_lr %>% 
  ungroup() %>% 
  mutate(learning_rate = as.numeric(learning_rate)) %>% 
  filter(test < 120) %>% 
  ggplot(aes(learning_rate, test)) +
  geom_point()

# Check learning curves
lr_mods[[rmse_lr$learning_rate[1]]] %>% 
  pull_eval() %>% 
  filter(mean < 120) %>% 
  ggplot(aes(iter, mean, color = set)) +
  geom_line() +
  geom_point()

# Set learning rate, tune tree specific parameters
grid <- grid_max_entropy(min_n(c(0, 50)), # min_child_weight
                         tree_depth(), # max_depth
                         size = 30)

tree_mods <- map2(grid$min_n, grid$tree_depth, ~{
  xgb.cv(
    data = X,
    label = Y,
    nrounds = 5000,
    objective = "reg:linear",
    early_stopping_rounds = 50, 
    nfold = 10,
    verbose = 0,
    params = list( 
      eta = as.numeric(rmse_lr$learning_rate[1]),
      max_depth = .x,
      min_child_weight = .y,
      nthread = 16
    ) 
  )  
}) 
```